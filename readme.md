# Crypto Trading System with ML-Powered Signals

This project implements a cryptocurrency trading system that uses real-time market trades ingested from Coinbase's Websockets API to decide whether to buy or sell BTC. To aid the RL model, the system also uses a ksqlDB stream to calculate signals such as volume and average price over a moving window. Currently, only paper trades are supported with a hardcoded $100,000 USD starting balance

## Features

- Real-time BTC-USD market trades ingestion from Coinbase
- Scalable data processing pipeline using Kafka
- Stream processing to generate trading signals with ksqlDB
- Trading bot the uses RL Based Deep-Q Network to decide whether to buy or sell BTC
- Backtesting capabilities on a flat file of historical data
- 1-step deployment with Docker Compose

## Architecture

The system consists of several microservices and components:

- **Ingestor Service**: Connects to Coinbase WebSocket API to receive real-time BTC-USD market trades, converts data types (strings to floats, timestamps to Unix milliseconds), and publishes formatted events to Kafka
- **Kafka & ksqlDB**: Message broker and stream processing for reliable data streaming and real-time analysis. ksqlDB processes trade events to generate signals like moving averages and volume metrics
- **Trader Service**: Python service that uses a trained Deep Q-Network model to make trading decisions based on the enriched trade data from Kafka. 

### System Diagram
```
                                   [ksqlDB]
                                     ↑ ↓ 
|Coinbase WebSocket| → [Ingestor] → [Kafka] → [Trader] → |Console Output|

                            [DQL Training Pipeline] 
```

## Data Flow

1. Ingestor service subscribes to Coinbase WebSocket feed for BTC-USD market trades
2. Raw market trades are published to Kafka topics
3. ksqlDB processes and enriches the data streams
4. The ML pipeline can:
   - Train new models using historical data from a flat file structured like backtest.txt
   - Backtest strategies using a file structured like backtest.txt
   - Deploy models to the trader service
5. Trader service uses the deployed ML models to:
   - Process real-time market data
   - Generate trading signals
   - Execute trades on the exchange

## Setup

### Prerequisites

- Docker and Docker Compose v2.x or higher
- Go 1.21 or higher
- Python 3.11 with pip

### Running the system

Start all the services:
```bash
docker-compose up
```

### ML Model Training

Open and run the Jupyter notebook:
```bash
jupyter notebook python/train_rl_agent.ipynb
```

## Configuration

The services can be configured through environment variables or configuration files:

- `config/ingestor_config.go`: Ingestor service configuration
- `config/trader_config.go`: Trader service configuration
- `python/config.py`: ML pipeline configuration

## Development

### Project Structure
```
.
├── cmd/                     # Application entry points
│   ├── ingestor/            # Ingestor service main
│   └── signaler_deprecated/ # Deprecated signaler service
├── internal/                # Private application code
│   ├── config/              # Service configuration
│   ├── ingestor/            # Ingestor service implementation
│   ├── kafka/               # Kafka producers and consumers
│   ├── signaler/            # Legacy signaler implementation
│   └── websocket/           # WebSocket client implementation
├── python/                  # ML and trading components
│   ├── train_rl_agent.ipynb # Model training notebook
│   ├── run_rl_agent.py      # Model deployment script
│   ├── model.zip            # Trained model artifacts (generated by the training notebook)
│   ├── trade_logs.txt       # Trading activity logs (generated by the trader service)
│   └── backtest.txt         # Historical data for training and backtesting
├── Dockerfile.ingestor      # Ingestor service container
├── Dockerfile.trader        # Trading service container
├── docker-compose.yml       # Service orchestration
├── go.mod                   # Go dependencies
├── go.sum                   # Go dependency checksums
├── ksqldb.sql               # Stream processing definitions
├── requirements.txt         # Python dependencies
```

## TODO

- [ ] Use ksqldb version 0.29.0 instead of latest
- [ ] Fix ksqldb.sql not running when ksqldb-server is started
  - Workaround: Manually run `docker exec -it ksqldb-server ksql` and paste the contents of ksqldb.sql



